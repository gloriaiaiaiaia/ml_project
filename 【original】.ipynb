{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "dbf2ff22-712b-4fc2-8687-99caa9bf07d8",
    "_uuid": "f57e59e8589cb94001d8673fb737a4e0d96852bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2805f6da-0df3-41f1-977a-547dac26a11f",
    "_uuid": "113b5b8f58952300f5a35a3999788e2d076beeab"
   },
   "source": [
    "This kernel is specifically is for Beginners who want's to experiment building CNN using Keras. By using this kernel, you can expect to get good score and also learn keras. \n",
    "Keras is simple frameworks where we can initialize the model and keep stacking the layers we want. It makes building deep neural networks very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "ea3f4874-a9aa-42f1-9605-b1784a6f48ba",
    "_uuid": "58c82d3b3c4b4305b388a6ac4eeca49d600f9105",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "804d3969-9035-4ceb-bb65-1b8549d729ec",
    "_uuid": "7a7f3af5ef279a9ed26c4d9ee764bd1fb4bdf10e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data.\n",
    "train = pd.read_json(\"train/processed/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "7b546aab-7b7d-4cde-91cc-e794fd4041bd",
    "_uuid": "2c18cf164fbbc6d1c29e9c668cbfcd7a1ea10824",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_json(\"test/processed/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e178779f-0698-47cd-9be5-50f9c9590089",
    "_uuid": "f5b6c2ba24e6bf5726f8551cdeeeaf931184c2bc"
   },
   "source": [
    "#Intro about the Data.\n",
    "\n",
    "Sentinet -1 sat is at about 680 Km above earth. Sending pulses of signals at a particular angle of incidence and then recoding it back. Basically those reflected signals are called backscatter. The data we have been given is backscatter coefficient which is the conventional form of backscatter coefficient given by:\n",
    "\n",
    "$σo (dB) = βo (dB) + 10log10 [ sin(ip) / sin (ic)] $\n",
    "\n",
    "where\n",
    "1. ip=is angle of incidence for a particular pixel\n",
    "2. 'ic ' is angle of incidence for center of the image\n",
    "3. K =constant.\n",
    "\n",
    "We have been given $σo$ directly in the data. \n",
    "###Now coming to the features of $σo$\n",
    "Basically σo varies with the surface on which the signal is scattered from. For example, for a particular angle of incidence, it varies like:\n",
    "*             WATER...........           SETTLEMENTS........           AGRICULTURE...........          BARREN........\n",
    "\n",
    "1.**HH:**     -27.001   ................                     2.70252       .................                -12.7952        ................    -17.25790909\n",
    "\n",
    "2.**HV: **      -28.035      ................            -20.2665             ..................          -21.4471       .................     -20.019\n",
    "\n",
    "As you can see, the HH component varies a lot but HV doesn't.\n",
    "**I don't have the data for scatter from ship, but being a metal object, it should vary differently as compared to ice object.**\n",
    "\n",
    "###WTF is HH HV?\n",
    "\n",
    "Ok, so this Sentinal Settalite is equivalent to RISTSAT(an Indian remote sensing Sat) and they only Transmit pings in H polarization, **AND NOT IN V polarization**.  Those H-pings gets scattered, objects change their polarization and returns as a mix of H and V.\n",
    "**Since Sentinel has only H-transmitter, return signals are of the form of HH and HV only**. Don't ask why VV is not given(because Sentinel don't have V-ping transmitter).\n",
    "\n",
    "Now coming to features, for the purpose of this demo code, I am extracting all two bands and taking avg of them as 3rd channel to create a 3-channel RGB equivalent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "829bf7db-fab1-4a2d-9562-0a37c6390d2a",
    "_uuid": "5292632717f11cd01c135dfabfd3cda9318cc639",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "#Create 3 bands having HH, HV and avg of both\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd7ea3c1-f039-445e-b5da-adfb608930d7",
    "_uuid": "1c65412c80ff504df19d55aa14093ebcd1028e6a"
   },
   "source": [
    "That's a cool looking iceberg we have. Remember, in radar data, the shape of the iceberg is going to be like a mountain as shown in here. Since this is not a actual image but scatter from radar, the shape is going to have peaks and distortions like these. The shape of the ship is going to be like a point, may be like a elongated point. From here the structural differences arise and we can exploit those differences using a CNN. It would be helpful if we can create composite images using the backscatter from radar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e2a6f33-992a-435e-be65-5ee614e6f7ba",
    "_uuid": "f907f993a1c63f4519872ecb381021c1a0dce2ca"
   },
   "source": [
    "That's a ship, looks like a elongated point. We don't have much resolution in images to visualize the shape of the ship. However CNN is here to help. There are few papers on ship iceberg classification like this:\n",
    "http://elib.dlr.de/99079/2/2016_BENTES_Frost_Velotto_Tings_EUSAR_FP.pdf\n",
    "However their data have much better resolution so I don't  feel that the CNN they used would be suitable here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c0c41e0-b95e-4129-a95e-06cd6808553d",
    "_uuid": "906b685c8ab2b91b3b9e4baffb65ad1c1aa028c3"
   },
   "source": [
    "Get back to building a CNN using Keras. Much better frameworks then others. You will enjoy for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "fb15bc53-becc-4e87-88ce-3bc99d45358d",
    "_uuid": "7a68a94f8c617209dfe56a58e291193e963d0f62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d7a4c0cc-0e96-46ea-960c-89bb80e11b56",
    "_uuid": "4602792c9d531903bd65c3b127a1e6be2c444b2d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define our model\n",
    "def getModel():\n",
    "    #Building the model\n",
    "    gmodel=Sequential()\n",
    "    #Conv Layer 1\n",
    "    gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    gmodel.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    gmodel.add(Dense(512))\n",
    "    gmodel.add(Activation('relu'))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    gmodel.add(Dense(256))\n",
    "    gmodel.add(Activation('relu'))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    gmodel.add(Dense(1))\n",
    "    gmodel.add(Activation('sigmoid'))\n",
    "\n",
    "    mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    gmodel.compile(loss='binary_crossentropy',\n",
    "                  optimizer=mypotim,\n",
    "                  metrics=['accuracy'])\n",
    "    gmodel.summary()\n",
    "    return gmodel\n",
    "\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \".model_weights.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1d690d4a-09ca-417c-8090-2aa417c514dd",
    "_uuid": "a883659e53709da950d04a4e5349c66d77a9422f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train=train['is_iceberg']\n",
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, target_train, random_state=1, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "d6bb750a-e882-4429-ad23-4392389f427f",
    "_uuid": "4e6dab11165b7d9515eb32b698851b260f0d941f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 73, 73, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 64)          73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,193\n",
      "Trainable params: 560,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1203 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "1203/1203 [==============================] - 43s - loss: 0.8806 - acc: 0.5062 - val_loss: 0.5959 - val_acc: 0.6359\n",
      "Epoch 2/20\n",
      "1203/1203 [==============================] - 42s - loss: 0.5691 - acc: 0.6475 - val_loss: 0.5584 - val_acc: 0.6883\n",
      "Epoch 3/20\n",
      "1203/1203 [==============================] - 43s - loss: 0.5215 - acc: 0.7232 - val_loss: 0.4854 - val_acc: 0.7556\n",
      "Epoch 4/20\n",
      "1203/1203 [==============================] - 38s - loss: 0.4492 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.8105\n",
      "Epoch 5/20\n",
      "1203/1203 [==============================] - 41s - loss: 0.4253 - acc: 0.8047 - val_loss: 0.4006 - val_acc: 0.8229\n",
      "Epoch 6/20\n",
      "1203/1203 [==============================] - 42s - loss: 0.4196 - acc: 0.7880 - val_loss: 0.3649 - val_acc: 0.8504\n",
      "Epoch 7/20\n",
      "1203/1203 [==============================] - 44s - loss: 0.3910 - acc: 0.8196 - val_loss: 0.3405 - val_acc: 0.8579\n",
      "Epoch 8/20\n",
      "1203/1203 [==============================] - 64s - loss: 0.3825 - acc: 0.8188 - val_loss: 0.4034 - val_acc: 0.7955\n",
      "Epoch 9/20\n",
      "1203/1203 [==============================] - 55s - loss: 0.3853 - acc: 0.8105 - val_loss: 0.3887 - val_acc: 0.8304\n",
      "Epoch 10/20\n",
      "1203/1203 [==============================] - 58s - loss: 0.3656 - acc: 0.8254 - val_loss: 0.3250 - val_acc: 0.8479\n",
      "Epoch 11/20\n",
      "1203/1203 [==============================] - 70s - loss: 0.3420 - acc: 0.8362 - val_loss: 0.3289 - val_acc: 0.8354\n",
      "Epoch 12/20\n",
      "1203/1203 [==============================] - 70s - loss: 0.3373 - acc: 0.8520 - val_loss: 0.2967 - val_acc: 0.8529\n",
      "Epoch 13/20\n",
      "1203/1203 [==============================] - 71s - loss: 0.3356 - acc: 0.8396 - val_loss: 0.3117 - val_acc: 0.8653\n",
      "Epoch 14/20\n",
      "1203/1203 [==============================] - 71s - loss: 0.3211 - acc: 0.8412 - val_loss: 0.3880 - val_acc: 0.8279\n",
      "Epoch 15/20\n",
      "1203/1203 [==============================] - 70s - loss: 0.3513 - acc: 0.8346 - val_loss: 0.2761 - val_acc: 0.8753\n",
      "Epoch 16/20\n",
      "1203/1203 [==============================] - 75s - loss: 0.2788 - acc: 0.8770 - val_loss: 0.2810 - val_acc: 0.8703\n",
      "Epoch 17/20\n",
      "1203/1203 [==============================] - 69s - loss: 0.3113 - acc: 0.8587 - val_loss: 0.3220 - val_acc: 0.8329\n",
      "Epoch 18/20\n",
      "1203/1203 [==============================] - 69s - loss: 0.2830 - acc: 0.8811 - val_loss: 0.3066 - val_acc: 0.8454\n",
      "Epoch 19/20\n",
      "1203/1203 [==============================] - 70s - loss: 0.2615 - acc: 0.8894 - val_loss: 0.2766 - val_acc: 0.8753\n",
      "Epoch 20/20\n",
      "1203/1203 [==============================] - 71s - loss: 0.2897 - acc: 0.8745 - val_loss: 0.3093 - val_acc: 0.8529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x198c47f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Without denoising, core features.\n",
    "import os\n",
    "gmodel=getModel()\n",
    "gmodel.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=30,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "079f0a8d-d2a5-4154-b37f-b425333e4ada",
    "_uuid": "0fa65f37d198cd6301376f179d9de0ccc1d40db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 6s     \n",
      "Test loss: 0.365493659367\n",
      "Test accuracy: 0.850374065432\n"
     ]
    }
   ],
   "source": [
    "gmodel.load_weights(filepath=file_path)\n",
    "score = gmodel.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "7cae1458-a566-4714-8b80-0b23fe88509c",
    "_uuid": "27f021784da863a2ad960a96b9c7394f25521802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 144s   \n"
     ]
    }
   ],
   "source": [
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , ((X_band_test_1+X_band_test_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "predicted_test=gmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "da3618f6-6e0a-475c-a390-7e17f5406c1a",
    "_kg_hide-output": false,
    "_uuid": "b34412c33fe8250df3285867d9a13e4bd08e8c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  641  iceburgs out of  8424  data.\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('sub.csv', index=False)\n",
    "# print(submission)\n",
    "ice = 0\n",
    "for i in range (len(test['id'])):\n",
    "    if (predicted_test.reshape((predicted_test.shape[0]))[i] >= 0.9):\n",
    "#         print(submission['id'][i],submission['is_iceberg'][i])\n",
    "        ice += 1\n",
    "print('There are ', ice,' iceburgs out of ', len(test['id']),' data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5140949-c867-43fd-baba-49e57bec44cf",
    "_uuid": "741f9696d91da6af29266fb199a6c2fb80d26dfe",
    "collapsed": true
   },
   "source": [
    "#### Conclusion\n",
    "To increase the score, I have tried Speckle filtering, Indicence angle normalization and other preprocessing and they don't seems to work.  You may try and see but for me they are not giving any good results.\n",
    "\n",
    "You can't be on top-10 using this kernel, so here is one beautiful peice of information. The test dataset contain 8000 images, We can exploit this. We can do pseudo labelling to increase the predictions. Here is the article related to that:\n",
    "https://towardsdatascience.com/simple-explanation-of-semi-supervised-learning-and-pseudo-labeling-c2218e8c769b\n",
    "\n",
    "Upvote if you liked this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "637e3662-38ac-4fa6-8065-48c8105026a9",
    "_uuid": "962411dc0d6a00c1730bfd22767542210c36f751",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
